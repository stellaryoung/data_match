{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "01 GAT_CORA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axKI79t1le8y",
        "colab_type": "text"
      },
      "source": [
        "#### GAT模型transductive learning(Cora数据集)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_64qOyPbwadf",
        "colab_type": "text"
      },
      "source": [
        "### Layers.py\n",
        "建立单层的GAT网络\n",
        "* 定义weight matrix对原有特征表示进行线性变化\n",
        "* 需要实现self-attention机制\n",
        "* 需要加权求和得到新的特征表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmJhuLjowmmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R09meWTpqMSd",
        "colab_type": "code",
        "outputId": "91a702d6-c78d-4c65-aea2-73899f07bd48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12TXU7iGsMoj",
        "colab_type": "text"
      },
      "source": [
        "**repeat的用法还是不理解**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQMKD7Vjlbn0",
        "colab_type": "code",
        "outputId": "99e3c67c-cfbe-4480-d47d-0b7830736861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "# 参数：sizes (torch.Size or python:int...) \n",
        "#– The number of times to repeat this tensor along each dimension\n",
        "print(x.shape)          # torch.Size([3])\n",
        "print(x.repeat(1,4))\n",
        "print(x.repeat(1,4).shape)    # torch.Size([1, 12])\n",
        "print(x.repeat(4, 2))\n",
        "print(x.repeat(4,2).shape)    # torch.Size([4, 6])\n",
        "print(x.repeat(4, 2, 1).shape) # torch.Size([4, 2, 3])?????\n",
        "print(x.repeat(4, 2, 1))    # 猜测最左边是第一个维度\n",
        "\n",
        "print(x.repeat(1, 2, 4).shape) # torch.Size([1, 2, 12])?????\n",
        "print(x.repeat(1, 2, 4))    # 猜测最左边是第一个维度"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n",
            "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]])\n",
            "torch.Size([1, 12])\n",
            "tensor([[1, 2, 3, 1, 2, 3],\n",
            "        [1, 2, 3, 1, 2, 3],\n",
            "        [1, 2, 3, 1, 2, 3],\n",
            "        [1, 2, 3, 1, 2, 3]])\n",
            "torch.Size([4, 6])\n",
            "torch.Size([4, 2, 3])\n",
            "tensor([[[1, 2, 3],\n",
            "         [1, 2, 3]],\n",
            "\n",
            "        [[1, 2, 3],\n",
            "         [1, 2, 3]],\n",
            "\n",
            "        [[1, 2, 3],\n",
            "         [1, 2, 3]],\n",
            "\n",
            "        [[1, 2, 3],\n",
            "         [1, 2, 3]]])\n",
            "torch.Size([1, 2, 12])\n",
            "tensor([[[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
            "         [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngtVfovz_b5p",
        "colab_type": "text"
      },
      "source": [
        "**如何将目标节点与邻居结点的特征拼接在一起？**\n",
        "***\n",
        "* step1:将所有可能的组合都拼接在一起\n",
        "\n",
        "* Graph Feature: $\\mathbf{H}=\\left\\{\\vec{h}_{1}, \\vec{h}_{2}, \\ldots, \\vec{h}_{N}\\right\\}^{T}$ （node_num,in_dim）\n",
        "\n",
        "* H.permute(1,N): (node_num,in_dim*node_num)\n",
        "\n",
        "$\\left\\{\\vec{h}_{1}, \\vec{h}_{1}, \\ldots, \\vec{h}_{1}\\right\\}$\n",
        "\n",
        "$\\left\\{\\vec{h}_{2}, \\vec{h}_{2}, \\ldots, \\vec{h}_{2}\\right\\}$\n",
        "\n",
        "——————省略号\n",
        "\n",
        "$\\left\\{\\vec{h}_{N}, \\vec{h}_{N}, \\ldots, \\vec{h}_{N}\\right\\}$\n",
        "\n",
        "**permute之后的矩阵每行将原结点特征重复N次**\n",
        "\n",
        "* H.permute(1,N).view(N*N,-1)，得到一个列向量，维度为($nodeNum^{2}$,in_dim)\n",
        "\n",
        "* a_input=torch.cat([H.repeat(1, N).view(N * N, -1),H.repeat(N, 1)],dim=1)\n",
        "\n",
        "上面就是得到所有结点向量表示的组合，（结点1，结点2）与（结点2，结点1）算是2种不同的组合。\n",
        "\n",
        "最终拼接的维度是($N^{2}$, in_dim+in_dim),每行是2个结点拼接的特征表示。\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOZToOsftxQl",
        "colab_type": "code",
        "outputId": "70653650-a992-478d-f8bb-2edd15713311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "torch.manual_seed(5) \n",
        "H = torch.LongTensor([[1,2,4,5],[4,3,2,9],[1,1,1,1]]) \n",
        "N = H.size()[0]\n",
        "print(N)\n",
        "print(H.shape)    # H: (node_num,out_dim)\n",
        "print(H.repeat(1,N))\n",
        "print(H.repeat(1,N).shape)\n",
        "print(H.repeat(N,1))\n",
        "print(H.repeat(N,1).shape)\n",
        "a_input=torch.cat([H.repeat(1, N).view(N * N, -1),H.repeat(N, 1)],dim=1)\n",
        "print(a_input)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "torch.Size([3, 4])\n",
            "tensor([[1, 2, 4, 5, 1, 2, 4, 5, 1, 2, 4, 5],\n",
            "        [4, 3, 2, 9, 4, 3, 2, 9, 4, 3, 2, 9],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "torch.Size([3, 12])\n",
            "tensor([[1, 2, 4, 5],\n",
            "        [4, 3, 2, 9],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 2, 4, 5],\n",
            "        [4, 3, 2, 9],\n",
            "        [1, 1, 1, 1],\n",
            "        [1, 2, 4, 5],\n",
            "        [4, 3, 2, 9],\n",
            "        [1, 1, 1, 1]])\n",
            "torch.Size([9, 4])\n",
            "tensor([[1, 2, 4, 5, 1, 2, 4, 5],\n",
            "        [1, 2, 4, 5, 4, 3, 2, 9],\n",
            "        [1, 2, 4, 5, 1, 1, 1, 1],\n",
            "        [4, 3, 2, 9, 1, 2, 4, 5],\n",
            "        [4, 3, 2, 9, 4, 3, 2, 9],\n",
            "        [4, 3, 2, 9, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 2, 4, 5],\n",
            "        [1, 1, 1, 1, 4, 3, 2, 9],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ninOSNlXrRX",
        "colab_type": "text"
      },
      "source": [
        "**激活函数**\n",
        "***\n",
        "torch.nn.ELU(alpha=1.0, inplace=False)\n",
        "\n",
        "**注意：**这个函数是element-wise operation,即元素中**逐个激活**\n",
        "\n",
        "$\\operatorname{ELU}(x)=\\max (0, x)+\\min (0, \\alpha *(\\exp (x)-1))$\n",
        "***\n",
        "\n",
        "torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
        "\n",
        "该函数也是element_wise operation:\n",
        "\n",
        "$\\text { LeakyReLU }(x)=\\max (0, x)+\\text { negative_slope } * \\min (0, x)$\n",
        "***\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHURgeTxFHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "  def __init__(self,in_dim,out_dim,dropout,alpha,concat=True):\n",
        "    super(GraphAttentionLayer,self).__init__()\n",
        "    # network structure parameters\n",
        "    # 这里使用self保存参数是为了方便forward函数使用\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "    self.dropout = dropout\n",
        "    self.alpha = alpha\n",
        "    self.concat = concat\n",
        "\n",
        "    # network module\n",
        "    # --linear transformation: weight matrix (in_dim,out_dim)\n",
        "    self.W = nn.Parameter(torch.zeros(size=(in_dim,out_dim)))\n",
        "    # -- feed-forward neural network: attention weight vector\n",
        "    self.a = nn.Parameter(torch.zeros(size=(2*out_dim,1)))\n",
        "    # activate function\n",
        "    self.leakyrelu = nn.LeakyReLU(negative_slope=self.alpha)\n",
        "\n",
        "    self.init_params() # initialzie parameters\n",
        "\n",
        "\n",
        "\n",
        "  def init_params(self):\n",
        "    nn.init.xavier_uniform_(self.W.data,gain=1.414) \n",
        "    # 这里的gain是如何计算出来的\n",
        "    nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
        "\n",
        "  # input: node feature representation\n",
        "  # adj: adjacency matrix\n",
        "  def forward(self,input,adj):\n",
        "    # input：(node_num,in_dim)\n",
        "    # adj: (node_num,node_num)\n",
        "    # H: (node_num,out_dim)\n",
        "    # linear transformation\n",
        "    H = torch.mm(input,self.W)\n",
        "    # N: number of graph nodes\n",
        "    N = H.size()[0]\n",
        "\n",
        "    # attention_input: all (target_node_representation) || (other node representation)\n",
        "    # two step is attention mechansim stage1: similiarity(score) calculation\n",
        "    attention_input=torch.cat([H.repeat(1, N).view(N * N, -1),H.repeat(N, 1)],dim=1).view(N,-1,2*self.out_dim)\n",
        "    attention_coefficient = self.leakyrelu(torch.matmul(attention_input,self.a).squeeze(2))\n",
        "    # attention_coefficient:[N,N],注意这里建立的权重要与邻接矩阵对应起来\n",
        "\n",
        "    # two step is to mask attention,just pay attention to neighbourhood node.\n",
        "    zero_matrix = -9e15*torch.ones_like(attention_coefficient)\n",
        "    # define zero vector, not true zero,but very small number\n",
        "    masked_coefficient = torch.where(adj>0,attention_coefficient,zero_matrix)\n",
        "    # masked_coefficient：[N,N]\n",
        "\n",
        "    # calculate attention weight\n",
        "    a_weight = F.softmax(masked_coefficient,dim=1)\n",
        "    a_weight = F.dropout(a_weight,self.dropout,training=self.training)\n",
        "    # self.training的来源是哪里？\n",
        "    # torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)\n",
        "    H_prime = torch.matmul(a_weight,H) \n",
        "    # H_prime:(node_num,out_dim)\n",
        "\n",
        "    if self.concat: # conact next layer\n",
        "      return F.elu(H_prime)\n",
        "    else:\n",
        "      return H_prime\n",
        "  # information of layer\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + ' (' + str(self.in_dim) + ' -> ' + str(self.out_dim) + ')'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cmqKqOYqKgS",
        "colab_type": "code",
        "outputId": "5432cf42-f8d6-46f5-8a96-4099c52c6f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "torch.manual_seed(5)\n",
        "test_feature = torch.randn((10,100))\n",
        "adj = torch.LongTensor([1,0,1,0,1,0,1,1,0,0]).repeat(10,1)\n",
        "print(adj.shape)\n",
        "\n",
        "layer_test = GraphAttentionLayer(in_dim=100,out_dim=10,dropout=0.5,alpha=0.01,concat=True)\n",
        "print(layer_test.__repr__()) \n",
        "test_output = layer_test(test_feature,adj)\n",
        "print(test_output.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "GraphAttentionLayer (100 -> 10)\n",
            "torch.Size([10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnvsB9ur-Oqx",
        "colab_type": "text"
      },
      "source": [
        "### model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxa_FZPaB-HN",
        "colab_type": "text"
      },
      "source": [
        "**API使用**\n",
        "***\n",
        "add_module(name, module)：添加子模块，可以使用self.name访问模块\n",
        "***\n",
        "普通的softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "* This function doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it’s faster and has better numerical properties).\n",
        "\n",
        "log_softmax：这个函数更适用于NLLLoss\n",
        "\n",
        "**上面2个函数使用要注意**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLAo2n0fmUw7",
        "colab_type": "text"
      },
      "source": [
        "**模型介绍：**\n",
        "* 模型分为2层，第一层是图网络层，第二层用于分类，先使用一个单独的注意力头去计算出C features(C是结点的种类数)，接着使用一个softmax激活。\n",
        "* 论文中提出给每层的输入引入0.6的dropout，解释是**在每个训练迭代中，每个节点都暴露在随机采样的邻域中**（at each training iteration, each node is exposed to a stochastically sampled neighborhood）\n",
        "* 论文中还提到在训练中不太准确的引入$L2$正则项，这一点该notebook中没有采纳，实验效果没有变差。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO-S83xg-N1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self,in_dim,hidden_dim,n_class,dropout,alpha,n_heads):\n",
        "    \"\"\"Dense version of GAT\"\"\"\n",
        "    super(GAT,self).__init__()\n",
        "    self.dropout = dropout\n",
        "    self.multi_attentions=[GraphAttentionLayer(in_dim,hidden_dim,dropout,alpha,concat=True) for _ in range(n_heads)]\n",
        "    # 将定义的多个图卷积层作为子模块添加进去\n",
        "    for i,layer in enumerate(self.multi_attentions):\n",
        "      self.add_module('attention_layer_{}'.format(i),self.multi_attentions[i])\n",
        "    # 最后网络的输出层\n",
        "    # 将multi-head attention的各个输出连接在一起作为输入,有什么意义？\n",
        "    # 这里之所以这样做是论文中EXPERIMENTAL SETUP中这样设置的\n",
        "    # 使用单独的注意力头计算，输出维度与结点种类数相同\n",
        "    self.out_layer=GraphAttentionLayer(hidden_dim*n_heads,n_class,dropout,alpha,False)\n",
        "\n",
        "\n",
        "  def forward(self,x,adj):\n",
        "    x = F.dropout(x,self.dropout,training=self.training)\n",
        "    # concatenate features [n_node,n_head*out_dim]\n",
        "    x = torch.cat([layer(x,adj) for layer in self.multi_attentions],dim=1)\n",
        "    x = F.dropout(x,self.dropout,training=self.training)\n",
        "    # 这里直接将拼接后的目标结点特征作为输入单个注意力头的输入\n",
        "    # 作者认为这是一种average操作\n",
        "    x = F.elu(self.out_layer(x,adj))\n",
        "    return F.log_softmax(x,dim=1)  # 输出结点概率分布"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSeqV1QPqqHC",
        "colab_type": "text"
      },
      "source": [
        "**测试GAT模型代码**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1vtnJ1hqjku",
        "colab_type": "code",
        "outputId": "c2e0fdb7-02d7-4e52-a9e0-ef6ca72d4bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "myGAT=GAT(in_dim=100,hidden_dim=10,n_class=3,dropout=0.5,alpha=0.1,n_heads=2)\n",
        "torch.manual_seed(5)\n",
        "test_feature = torch.randn((10,100))\n",
        "adj = torch.LongTensor([1,0,1,0,1,0,1,1,0,0]).repeat(10,1)\n",
        "print(adj.shape)\n",
        "test_output = myGAT(test_feature,adj)\n",
        "print(test_output.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwrMx0ndGMo9",
        "colab_type": "text"
      },
      "source": [
        "### 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj4ebQO3Zqm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import glob   # lists of files matching given patterns, just like the Unix shell\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ulxSPc-GbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpqUYKBaKROS",
        "colab_type": "text"
      },
      "source": [
        "这里的mx是$\\tilde{A}=A+I$,即邻接矩阵+单位矩阵\n",
        "\n",
        "rowsum:$\\tilde{A}$的度矩阵$\\tilde{D}$\n",
        "\n",
        "r_mat_inv_sqrt:$\\tilde{D}^{-\\frac{1}{2}}$\n",
        "\n",
        "最终结果：$(\\tilde{A}\\tilde{D}^{-\\frac{1}{2}})^{T}\\tilde{D}^{-\\frac{1}{2}}$\n",
        "\n",
        "实际论文中表达：$\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$\n",
        "\n",
        "这是对预处理步骤，原论文称为**renormalization trick**，为了alleviate数值不稳定以及梯度爆炸和消失问题\n",
        "\n",
        "见论文：SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS\n",
        "\n",
        "此处代码疑问：为什么与原论文有差异？？？\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5HjHz9sH4PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "    # 这里的transpose()是转置操作"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcTw38ZFVFk0",
        "colab_type": "text"
      },
      "source": [
        "公式表达：$\\tilde{D}^{-1}\\tilde{A}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOWFFtFXJB6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GCN-pytorch代码的实现，这里作为对比不使用\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    # 这里之所以是行，是因为每一行都是一个特征\n",
        "    rowsum = np.array(mx.sum(1))  # 得到所有元素的和\n",
        "    # print(rowsum.shape)   # (2708, 1)\n",
        "    r_inv = np.power(rowsum, -1).flatten()  # 取倒数,0的倒数为inf,flatten降维成列表\n",
        "    r_inv[np.isinf(r_inv)] = 0.    # 将inf替换为0\n",
        "    r_mat_inv = sp.diags(r_inv)    # 使用r_inv创立对角矩阵\n",
        "    mx = r_mat_inv.dot(mx)         # 将对角矩阵乘以原矩阵进行特征化\n",
        "    return mx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRclfuhBVx2w",
        "colab_type": "text"
      },
      "source": [
        "**这里搞不懂为什么要normalize features???**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUOUWP5uHxH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMAk8Y8uHxUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path=\"./data/cora/\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize_features(features)\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "    print(\"finishing load datasets!\")\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSTEVO8dWH2m",
        "colab_type": "code",
        "outputId": "08cdb5ad-a252-4f51-c069-6208bb5a9b37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "path='/content/drive/My Drive/GNN/data/cora/'\n",
        "adj, features, labels, idx_train, idx_val, idx_test=load_data(path=path)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "finishing load datasets!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgpjD8O6bO6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(adj.shape)\n",
        "print(features.shape)\n",
        "print(labels.shape)\n",
        "print(idx_train.shape)  # 140,包含0-139的值\n",
        "print(idx_train)  \n",
        "print(idx_val.shape)   # 300\n",
        "print(idx_val)      # 包含200-499各种值\n",
        "print(idx_test.shape)  # 1000\n",
        "# 包含500-1499范围的各种值"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl_rFAH-XAfy",
        "colab_type": "text"
      },
      "source": [
        "### train.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslL5jS0XU1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed=72\n",
        "epochs=10000\n",
        "lr=0.005\n",
        "weight_decay=5e-4\n",
        "hidden=8\n",
        "nb_heads=8\n",
        "dropout=0.6\n",
        "alpha=0.2\n",
        "patience=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcr4SgagXANj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model=GAT(in_dim=features.shape[1],\n",
        "    hidden_dim=hidden,\n",
        "    n_class=int(labels.max())+1,\n",
        "    dropout=dropout,\n",
        "    alpha=alpha,\n",
        "    n_heads=nb_heads)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay) # 模型参数优化器"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HRYd939Q5zf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ae4e0f5f-4bb7-40cf-e0fc-7cbe52826af5"
      },
      "source": [
        "print(labels[idx_train])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 6, 6, 2, 2, 5, 0, 0, 5, 0, 4, 2, 3, 0, 0, 6, 0, 0, 0, 5, 6, 4, 2,\n",
            "        6, 5, 0, 2, 0, 1, 3, 0, 3, 0, 6, 6, 0, 6, 4, 6, 5, 2, 4, 0, 3, 6, 0, 3,\n",
            "        2, 3, 3, 2, 5, 4, 5, 4, 5, 6, 0, 0, 6, 3, 2, 6, 3, 4, 3, 2, 0, 3, 5, 2,\n",
            "        0, 6, 2, 3, 2, 0, 5, 5, 0, 2, 6, 0, 2, 6, 1, 1, 5, 5, 0, 0, 6, 2, 2, 2,\n",
            "        3, 0, 5, 2, 4, 0, 0, 6, 0, 2, 1, 2, 2, 5, 0, 0, 0, 0, 5, 2, 3, 2, 6, 1,\n",
            "        4, 0, 0, 0, 0, 0, 2, 0, 6, 4, 5, 2, 6, 0, 0, 1, 1, 2, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlGPGOsiull",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 这里的cuda()是将所有张量转换到GPU计算\n",
        "model.cuda()\n",
        "features = features.cuda()\n",
        "adj = adj.cuda()\n",
        "labels = labels.cuda()\n",
        "idx_train = idx_train.cuda()\n",
        "idx_val = idx_val.cuda()\n",
        "idx_test = idx_test.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwwrgJzCtEiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukMfh4imkslA",
        "colab_type": "code",
        "outputId": "a4f3d5a0-d06f-4cb9-e5e5-966ba24d09a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# features, adj, labels = Variable(features), Variable(adj), Variable(labels) # 这一步不需要\n",
        "# 老版本pytorch代码需要用到Varialbe，新版本基本不需要"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUymhDdubb2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = '/content/sample_data/log'\n",
        "writer = SummaryWriter(log_dir=log_dir + '/' + time.strftime('%H:%M:%S', time.gmtime()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO-4ubNIo4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 这里features,adj作为全局变量加载\n",
        "# 注意此处的反向传播方法\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()   \n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    print(output.shape)    # torch.Size([2708, 7])\n",
        "    # train model: calculate loss value\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "\n",
        "    loss_train.backward() \n",
        "    optimizer.step()   \n",
        "\n",
        "    # evaluate model\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "\n",
        "    # print and recored information\n",
        "    train_loss = loss_train.detach().item()\n",
        "    dev_loss = loss_val.detach().item()\n",
        "    train_acc = acc_train.detach().item()\n",
        "    dev_acc = acc_val.detach().item()\n",
        "    time_cost = time.time() - t\n",
        "\n",
        "    # print('Epoch: {:04d}'.format(epoch+1),\n",
        "    #       'loss_train: {:.4f}'.format(train_loss),\n",
        "    #       'acc_train: {:.4f}'.format(train_acc),\n",
        "    #       'loss_val: {:.4f}'.format(dev_loss),\n",
        "    #       'acc_val: {:.4f}'.format(dev_acc),\n",
        "    #       'time: {:.4f}s'.format(time_cost))\n",
        "    c = epoch + 1\n",
        "    writer.add_scalar(\"loss/train\", train_loss,c)\n",
        "    writer.add_scalar(\"loss/dev\", dev_loss,c)\n",
        "    writer.add_scalar(\"acc/train\", train_acc,c)\n",
        "    writer.add_scalar(\"acc/dev\", dev_acc,c)\n",
        "    writer.add_scalar(\"epoch_time\",time_cost,c)\n",
        "\n",
        "    return dev_loss   # determine the model effect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxFWWqh_oFe-",
        "colab_type": "code",
        "outputId": "8cf8b827-2a05-4814-cd9d-8a3cfab8620d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# output:结点的概率分布矩阵\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "# output = torch.tensor([[0.4,0.6],[0.7,0.3],[0.1,0.9]])\n",
        "# labels = torch.LongTensor([0,1,1])\n",
        "# preds = output.max(1)[1].type_as(labels)\n",
        "# test = output.max(1)\n",
        "\n",
        "def GPU_info(empty=False):\n",
        "  print(\"memory allocated is {}\".format(torch.cuda.memory_allocated()))\n",
        "  print(\"cached memory is {}\".format(torch.cuda.memory_cached()))\n",
        "  if empty:  \n",
        "    print(\"releases all unused cached memory from PyTorch\")\n",
        "    torch.cuda.empty_cache()\n",
        "GPU_info(empty=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "memory allocated is 5076528640\n",
            "cached memory is 6782189568\n",
            "releases all unused cached memory from PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sq9hQx6fWm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dev_loss_list = []\n",
        "model_dir = '/content/sample_data/model/'\n",
        "bad_counter = 0 \n",
        "best_epoch = 1 \n",
        "best_dev_loss = float('inf') \n",
        "t_total = time.time()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  dev_loss = train(epoch)\n",
        "  dev_loss_list.append(dev_loss)\n",
        "  if dev_loss<best_dev_loss:\n",
        "    best_dev_loss = dev_loss\n",
        "    torch.save(model.state_dict(),model_dir+'{}.pkl'.format(epoch))\n",
        "    best_epoch = epoch\n",
        "  else:\n",
        "    bad_counter += 1\n",
        "  if bad_counter == 100:\n",
        "    break\n",
        "  # remove save model file before best epoch\n",
        "  files = glob.glob(model_dir+'*.pkl')\n",
        "  for file in files:\n",
        "    tmp = file.split('/')[-1]\n",
        "    tmp = tmp.split('.')[0]\n",
        "    epoch_nb = int(tmp)\n",
        "    if epoch_nb < best_epoch:\n",
        "      os.remove(file)\n",
        "  \n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "print(\"best epoch is {}\".format(best_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjUIiftRe6WO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e4b8f656-0baf-4277-e632-58c416600202"
      },
      "source": [
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load(model_dir+'{}.pkl'.format(best_epoch)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 433th epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3nmA2tfKPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09225ff2-d29d-4070-cda9-29e15955901f"
      },
      "source": [
        "model.eval()\n",
        "output = model(features, adj)\n",
        "loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "test_loss = loss_test.detach().item()\n",
        "test_acc = acc_test.detach().item()\n",
        "print(\"Test set results:\",\"loss= {:.4f}\".format(test_loss),\"accuracy= {:.4f}\".format(test_acc)) \n",
        "# 准确率是0.8410，原论文是83.0+(-)0.7"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set results: loss= 0.6890 accuracy= 0.8410\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN67V5LUhOsB",
        "colab_type": "text"
      },
      "source": [
        "### 总结\n",
        "关于模型\n",
        "* 模型是GAT模型，出自论文GRAPH ATTENTION NETWORKS-ICLR-2018\n",
        "***\n",
        "关于本实验\n",
        "* 本实验数据集是cora数据集，目的是用于评估网络的transductive learning\n",
        "\n",
        "* cora数据集共有2708个结点，由于是transuductive learning，所有结点\n",
        "的特征都会作为网络的输入，但该notebook实际训练时，使用140个结点用于参数优化，使用300个结点作为dev,使用1000个结点作为test,**这里原论文中是使用500个作为dev，并且训练时数据每一类结点都为20个，7类共140个，其余的基本一致**\n",
        "* 模型共2层，第一层时图网络层，注意头为8个，每个注意力维度为8，所以最终经过第一层网络输出的结点维度为64维，\n"
      ]
    }
  ]
}