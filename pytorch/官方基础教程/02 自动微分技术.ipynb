{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [自动微分官方参考链接](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOGRAD: AUTOMATIC DIFFERENTIATION（自动微分技术）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.Tensor的特点:**\n",
    "* set its attribute **.requires_grad** as True,开始追踪上面的所有操作\n",
    "* finish your computation you can call **.backward()** ,自动完成所有的梯度计算  \n",
    "* **.grad** attribute会累计所有的gradient\n",
    "* stop a tensor from tracking history,**.detach()** to detach it from the computation history, and to prevent future computation from being tracked\n",
    "* **评估模型的重要技巧：**wrap the code block in with **torch.no_grad()**. This can be particularly helpful when evaluating a model.模型已经有训练好的参数，不需要求导数，因此可以关闭导数。\n",
    "                 \n",
    "**还有一个类对自动微分实现十分重要,该类名字是 Function**\n",
    "\n",
    "类Tensor与Function相互关联并建立一个无环图，图中编码了一个完整的计算历史。\n",
    "每个Tensor都有.grad_fn属性用于引用Function,这个Function创建了Tensor(except for Tensors created by the user - their grad_fn is None)\n",
    "\n",
    "**导数的计算：**\n",
    "\n",
    "   If you want to compute the derivatives, you can call .backward() on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n",
    "   \n",
    "**常用的属性**\n",
    "* **a.requires_grad_(True)**：开启梯度操作记录, .requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given.\n",
    "\n",
    "**Gradients**\n",
    "\n",
    "* **out.backward()**：进行梯度计算\n",
    "* **x.grad**:查看计算的梯度值\n",
    "* **关闭自动梯度记录的方法**：2种\n",
    "\n",
    "**自动梯度计算数学上说明**\n",
    "有vector valued function $\\vec{y}=f(\\vec{x})$，then the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\\end{split}\n",
    "\n",
    "\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{array}\\right)\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)   # 追踪tensor上的所有操作\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2   # y was created as a result of an operation, so it has a grad_fn.\n",
    "print(y)    # grad_fn:y是通过张量x进行加法操作得到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "print(z)\n",
    "out = z.mean()\n",
    "print(z, out)    # 张量z先通过乘法操作。然后再取平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打开梯度操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)   # 这里的out是一个one item tensor\n",
    "out.backward()  # 进行梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print gradients d(out)/dx:得出梯度的计算结果**\n",
    "***\n",
    "\n",
    "You should have got a matrix of ``4.5``. Let’s call the ``out``\n",
    "*Tensor* “$o$”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $o = \\frac{1}{4}\\sum_i z_i \\quad z_i = 3(x_i+2)^2$ \n",
    "### $\\frac{\\partial o}{\\partial z_i} = \\frac{1}{4} \\quad \\frac{\\partial z_i}{\\partial x_i} = 6(x_i+2) \\quad \\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$\n",
    "### $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)    # 计算最终结果对x求导的值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000291CBE10AC8>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)   # 开启梯度操作记录\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[norm()的作用](https://blog.csdn.net/devcy/article/details/89218480)\n",
    "### 不知道这个例子是啥意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.norm of tensor([ 3.2700, -2.3328,  0.4107])>\n",
      "<bound method Tensor.norm of tensor([ 6.5400, -4.6656,  0.8213])>\n",
      "<bound method Tensor.norm of tensor([13.0801, -9.3312,  1.6426])>\n",
      "<bound method Tensor.norm of tensor([ 26.1602, -18.6625,   3.2853])>\n",
      "<bound method Tensor.norm of tensor([ 52.3204, -37.3250,   6.5706])>\n",
      "<bound method Tensor.norm of tensor([104.6407, -74.6500,  13.1411])>\n",
      "<bound method Tensor.norm of tensor([ 209.2814, -149.2999,   26.2823])>\n",
      "<bound method Tensor.norm of tensor([ 418.5628, -298.5999,   52.5645])>\n",
      "tensor([ 837.1257, -597.1997,  105.1291], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:    # data (array_like) – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types\n",
    "    print(y.data.norm)\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6350, -1.1664,  0.2053], requires_grad=True)\n",
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop autograd from tracking history on Tensors\n",
    "* .requires_grad=True \n",
    "* by wrapping the code block in with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
